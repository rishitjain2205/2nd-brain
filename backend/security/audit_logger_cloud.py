"""
Cloud Audit Logger - Ships logs to external immutable storage

SECURITY IMPROVEMENTS:
‚úÖ Ships logs to AWS CloudWatch, Datadog, Splunk, or Azure Monitor
‚úÖ Logs are immutable (cannot be deleted by compromised server)
‚úÖ Real-time shipping (attacker can't delete before upload)
‚úÖ Fallback to local if cloud unavailable
‚úÖ Automatic retry with exponential backoff

SUPPORTED BACKENDS:
- AWS CloudWatch Logs
- Datadog
- Splunk HEC (HTTP Event Collector)
- Azure Monitor (Log Analytics)
- Local file (fallback)
"""

import json
import logging
import os
import time
import hashlib
import hmac
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


class CloudAuditLogger:
    """
    Audit logger that ships logs to external immutable storage

    This prevents log tampering/deletion by attackers with filesystem access
    """

    def __init__(
        self,
        organization_id: str = None,
        backend: str = 'local',  # 'cloudwatch', 'datadog', 'splunk', 'azure', 'local'
        encrypt: bool = True,
        fallback_to_local: bool = True
    ):
        """
        Initialize cloud audit logger

        Args:
            organization_id: Organization ID for multi-tenant isolation
            backend: Cloud backend ('cloudwatch', 'datadog', 'splunk', 'azure', 'local')
            encrypt: Whether to encrypt audit logs
            fallback_to_local: Write to local file if cloud shipping fails
        """
        self.organization_id = organization_id or os.getenv('ORGANIZATION_ID', 'default')
        self.backend = backend.lower()
        self.encrypt = encrypt
        self.fallback_to_local = fallback_to_local

        # HMAC secret for tamper detection
        if encrypt:
            hmac_secret = os.getenv('AUDIT_HMAC_SECRET')
            if not hmac_secret or hmac_secret == 'default_hmac_secret_change_in_production':
                raise ValueError(
                    "‚ö†Ô∏è SECURITY ERROR: AUDIT_HMAC_SECRET environment variable must be set!\n"
                    "Generate one with: python3 -c \"import secrets; print(secrets.token_hex(32))\"\n"
                    "Then set: export AUDIT_HMAC_SECRET='<generated_value>'"
                )
            self.hmac_secret = hmac_secret.encode()
        else:
            self.hmac_secret = None

        # Initialize backend
        self._init_backend()

        # Local fallback directory
        if fallback_to_local:
            self.log_dir = Path("data/audit_logs") / self.organization_id
            self.log_dir.mkdir(parents=True, exist_ok=True)
            self.logger = self._setup_local_logger()

        # HTTP session with retry logic
        self.session = self._create_session()

        print(f"‚úì Cloud audit logger initialized (backend: {self.backend}, org: {self.organization_id})")

    def _init_backend(self):
        """Initialize cloud backend"""
        if self.backend == 'cloudwatch':
            self.cloudwatch_log_group = os.getenv('AWS_CLOUDWATCH_LOG_GROUP', '/2ndbrain/audit')
            self.cloudwatch_log_stream = f"{self.organization_id}-{datetime.now().strftime('%Y-%m-%d')}"
            self.aws_region = os.getenv('AWS_REGION', 'us-east-1')

            try:
                import boto3
                self.cloudwatch_client = boto3.client('logs', region_name=self.aws_region)
                # Create log group if it doesn't exist
                try:
                    self.cloudwatch_client.create_log_group(logGroupName=self.cloudwatch_log_group)
                except self.cloudwatch_client.exceptions.ResourceAlreadyExistsException:
                    pass
                # Create log stream
                try:
                    self.cloudwatch_client.create_log_stream(
                        logGroupName=self.cloudwatch_log_group,
                        logStreamName=self.cloudwatch_log_stream
                    )
                except self.cloudwatch_client.exceptions.ResourceAlreadyExistsException:
                    pass
                print(f"  ‚úì CloudWatch configured: {self.cloudwatch_log_group}/{self.cloudwatch_log_stream}")
            except ImportError:
                raise ImportError("boto3 not installed. Install with: pip install boto3")

        elif self.backend == 'datadog':
            self.datadog_api_key = os.getenv('DATADOG_API_KEY')
            if not self.datadog_api_key:
                raise ValueError("DATADOG_API_KEY environment variable required")
            self.datadog_site = os.getenv('DATADOG_SITE', 'datadoghq.com')
            self.datadog_url = f"https://http-intake.logs.{self.datadog_site}/api/v2/logs"
            print(f"  ‚úì Datadog configured: {self.datadog_url}")

        elif self.backend == 'splunk':
            self.splunk_hec_url = os.getenv('SPLUNK_HEC_URL')
            self.splunk_hec_token = os.getenv('SPLUNK_HEC_TOKEN')
            if not self.splunk_hec_url or not self.splunk_hec_token:
                raise ValueError("SPLUNK_HEC_URL and SPLUNK_HEC_TOKEN environment variables required")
            print(f"  ‚úì Splunk HEC configured: {self.splunk_hec_url}")

        elif self.backend == 'azure':
            self.azure_workspace_id = os.getenv('AZURE_LOG_ANALYTICS_WORKSPACE_ID')
            self.azure_shared_key = os.getenv('AZURE_LOG_ANALYTICS_SHARED_KEY')
            if not self.azure_workspace_id or not self.azure_shared_key:
                raise ValueError("AZURE_LOG_ANALYTICS_WORKSPACE_ID and AZURE_LOG_ANALYTICS_SHARED_KEY required")
            self.azure_log_type = "2ndBrainAudit"
            print(f"  ‚úì Azure Monitor configured: {self.azure_workspace_id}")

        elif self.backend == 'local':
            print("  ‚úì Local file backend (not tamper-proof!)")

        else:
            raise ValueError(f"Unsupported backend: {self.backend}")

    def _create_session(self) -> requests.Session:
        """Create HTTP session with retry logic"""
        session = requests.Session()

        # Retry strategy: 3 retries with exponential backoff
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,  # 1s, 2s, 4s
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )

        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        return session

    def _setup_local_logger(self) -> logging.Logger:
        """Set up local file logger (fallback)"""
        logger = logging.getLogger(f"cloud_audit_logger_{self.organization_id}")
        logger.setLevel(logging.INFO)

        # Daily log files
        today = datetime.now().strftime("%Y-%m-%d")
        log_file = self.log_dir / f"audit_{today}.jsonl"

        handler = logging.FileHandler(log_file)
        handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(message)s')
        handler.setFormatter(formatter)

        logger.addHandler(handler)
        logger.propagate = False

        return logger

    def _sign_log_entry(self, log_entry: Dict[str, Any]) -> str:
        """Create HMAC signature for log entry"""
        if not self.hmac_secret:
            return None

        canonical = json.dumps(log_entry, sort_keys=True)
        signature = hmac.new(
            self.hmac_secret,
            canonical.encode(),
            hashlib.sha256
        ).hexdigest()

        return signature

    def _ship_to_cloudwatch(self, log_entry: Dict[str, Any]) -> bool:
        """Ship log to AWS CloudWatch"""
        try:
            self.cloudwatch_client.put_log_events(
                logGroupName=self.cloudwatch_log_group,
                logStreamName=self.cloudwatch_log_stream,
                logEvents=[{
                    'timestamp': int(time.time() * 1000),
                    'message': json.dumps(log_entry)
                }]
            )
            return True
        except Exception as e:
            print(f"  ‚ö†Ô∏è CloudWatch shipping failed: {e}")
            return False

    def _ship_to_datadog(self, log_entry: Dict[str, Any]) -> bool:
        """Ship log to Datadog"""
        try:
            headers = {
                'DD-API-KEY': self.datadog_api_key,
                'Content-Type': 'application/json'
            }

            payload = {
                'ddsource': '2ndbrain',
                'ddtags': f'organization:{self.organization_id}',
                'service': '2ndbrain-api',
                'message': log_entry
            }

            response = self.session.post(
                self.datadog_url,
                headers=headers,
                json=payload,
                timeout=5
            )

            response.raise_for_status()
            return True

        except Exception as e:
            print(f"  ‚ö†Ô∏è Datadog shipping failed: {e}")
            return False

    def _ship_to_splunk(self, log_entry: Dict[str, Any]) -> bool:
        """Ship log to Splunk HEC"""
        try:
            headers = {
                'Authorization': f'Splunk {self.splunk_hec_token}',
                'Content-Type': 'application/json'
            }

            payload = {
                'event': log_entry,
                'sourcetype': '2ndbrain:audit',
                'source': '2ndbrain-api',
                'index': 'security'
            }

            response = self.session.post(
                self.splunk_hec_url,
                headers=headers,
                json=payload,
                timeout=5,
                verify=True  # Verify SSL
            )

            response.raise_for_status()
            return True

        except Exception as e:
            print(f"  ‚ö†Ô∏è Splunk shipping failed: {e}")
            return False

    def _ship_to_azure(self, log_entry: Dict[str, Any]) -> bool:
        """Ship log to Azure Monitor (Log Analytics)"""
        try:
            import hashlib
            import hmac
            import base64

            # Build signature
            rfc1123date = datetime.utcnow().strftime('%a, %d %b %Y %H:%M:%S GMT')
            body = json.dumps(log_entry)
            content_length = len(body)

            string_to_hash = f"POST\n{content_length}\napplication/json\nx-ms-date:{rfc1123date}\n/api/logs"
            bytes_to_hash = bytes(string_to_hash, encoding="utf-8")
            decoded_key = base64.b64decode(self.azure_shared_key)
            encoded_hash = base64.b64encode(
                hmac.new(decoded_key, bytes_to_hash, digestmod=hashlib.sha256).digest()
            ).decode()

            authorization = f"SharedKey {self.azure_workspace_id}:{encoded_hash}"

            # Send request
            url = f"https://{self.azure_workspace_id}.ods.opinsights.azure.com/api/logs?api-version=2016-04-01"

            headers = {
                'content-type': 'application/json',
                'Authorization': authorization,
                'Log-Type': self.azure_log_type,
                'x-ms-date': rfc1123date
            }

            response = self.session.post(url, headers=headers, data=body, timeout=5)
            response.raise_for_status()
            return True

        except Exception as e:
            print(f"  ‚ö†Ô∏è Azure Monitor shipping failed: {e}")
            return False

    def _write_local_fallback(self, log_entry: Dict[str, Any]):
        """Write to local file as fallback"""
        if self.fallback_to_local and self.logger:
            self.logger.info(json.dumps(log_entry))

    def log_event(
        self,
        action: str,
        user_id: str = None,
        success: bool = True,
        metadata: Dict[str, Any] = None,
        **kwargs
    ):
        """
        Log audit event

        Args:
            action: Type of action (rag_query, classification, etc.)
            user_id: User who performed the action
            success: Whether the action succeeded
            metadata: Additional metadata
            **kwargs: Additional fields
        """
        # Build log entry
        log_entry = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "organization_id": self.organization_id,
            "action": action,
            "user_id": user_id,
            "success": success,
            "metadata": metadata or {},
            **kwargs
        }

        # Sign log entry
        signature = self._sign_log_entry(log_entry)
        if signature:
            log_entry["signature"] = signature

        # Ship to cloud backend
        shipped = False
        if self.backend == 'cloudwatch':
            shipped = self._ship_to_cloudwatch(log_entry)
        elif self.backend == 'datadog':
            shipped = self._ship_to_datadog(log_entry)
        elif self.backend == 'splunk':
            shipped = self._ship_to_splunk(log_entry)
        elif self.backend == 'azure':
            shipped = self._ship_to_azure(log_entry)

        # Fallback to local if cloud shipping failed
        if not shipped and self.fallback_to_local:
            self._write_local_fallback(log_entry)

    def log_rag_query(
        self,
        user_id: str,
        model_deployment: str,
        query_hash: str,
        response_hash: str,
        sanitized: bool = True,
        success: bool = True
    ):
        """Log RAG query"""
        self.log_event(
            action="rag_query",
            user_id=user_id,
            model_deployment=model_deployment,
            query_hash=query_hash,
            response_hash=response_hash,
            data_sanitized=sanitized,
            success=success
        )

    def log_authentication(
        self,
        user_id: str,
        method: str,  # 'jwt', 'api_key', 'saml'
        success: bool,
        ip_address: str = None,
        user_agent: str = None,
        error: str = None
    ):
        """Log authentication attempt"""
        self.log_event(
            action="authentication",
            user_id=user_id,
            auth_method=method,
            success=success,
            ip_address=ip_address,
            user_agent=user_agent,
            error=error
        )

    def log_data_access(
        self,
        user_id: str,
        resource_type: str,  # 'document', 'user_profile', etc.
        resource_id: str,
        operation: str,  # 'read', 'write', 'delete'
        success: bool
    ):
        """Log data access"""
        self.log_event(
            action="data_access",
            user_id=user_id,
            resource_type=resource_type,
            resource_id=resource_id,
            operation=operation,
            success=success
        )

    def log_security_event(
        self,
        event_type: str,  # 'sql_injection_attempt', 'rate_limit_exceeded', etc.
        severity: str,  # 'low', 'medium', 'high', 'critical'
        details: Dict[str, Any],
        ip_address: str = None,
        user_id: str = None
    ):
        """Log security event"""
        self.log_event(
            action="security_event",
            event_type=event_type,
            severity=severity,
            details=details,
            ip_address=ip_address,
            user_id=user_id,
            success=False  # Security events are always "failures"
        )


# Global cloud audit logger instance
_cloud_audit_logger_instance = None


def get_cloud_audit_logger(
    organization_id: str = None,
    backend: str = None
) -> CloudAuditLogger:
    """
    Get global cloud audit logger instance

    Args:
        organization_id: Organization ID
        backend: Cloud backend (or from env AUDIT_LOG_BACKEND)

    Returns:
        CloudAuditLogger instance
    """
    global _cloud_audit_logger_instance

    if _cloud_audit_logger_instance is None:
        if backend is None:
            backend = os.getenv('AUDIT_LOG_BACKEND', 'local')

        _cloud_audit_logger_instance = CloudAuditLogger(
            organization_id=organization_id,
            backend=backend
        )

    return _cloud_audit_logger_instance


if __name__ == "__main__":
    print("="*80)
    print("CLOUD AUDIT LOGGER - TESTING")
    print("="*80)

    # Test with local backend (no cloud credentials needed)
    logger = CloudAuditLogger(
        organization_id="test_org",
        backend='local',
        encrypt=False  # Skip HMAC for demo
    )

    print("\n1Ô∏è‚É£  Logging RAG query...")
    logger.log_rag_query(
        user_id="user123",
        model_deployment="gpt-5-chat",
        query_hash="abc123",
        response_hash="def456",
        sanitized=True,
        success=True
    )
    print("   ‚úÖ Logged")

    print("\n2Ô∏è‚É£  Logging authentication...")
    logger.log_authentication(
        user_id="user123",
        method="jwt",
        success=True,
        ip_address="192.168.1.1",
        user_agent="Mozilla/5.0"
    )
    print("   ‚úÖ Logged")

    print("\n3Ô∏è‚É£  Logging security event...")
    logger.log_security_event(
        event_type="sql_injection_attempt",
        severity="critical",
        details={"payload": "'; DROP TABLE users; --", "blocked": True},
        ip_address="192.168.1.100",
        user_id="anonymous"
    )
    print("   ‚úÖ Logged")

    print("\n" + "="*80)
    print("‚úÖ CLOUD AUDIT LOGGER WORKING!")
    print("="*80)
    print("\nüìù CONFIGURATION:")
    print("  Set AUDIT_LOG_BACKEND to: cloudwatch, datadog, splunk, azure, or local")
    print("\n‚òÅÔ∏è  CLOUD BACKENDS:")
    print("  ‚Ä¢ CloudWatch: Set AWS_CLOUDWATCH_LOG_GROUP, AWS_REGION (requires boto3)")
    print("  ‚Ä¢ Datadog: Set DATADOG_API_KEY, DATADOG_SITE")
    print("  ‚Ä¢ Splunk: Set SPLUNK_HEC_URL, SPLUNK_HEC_TOKEN")
    print("  ‚Ä¢ Azure: Set AZURE_LOG_ANALYTICS_WORKSPACE_ID, AZURE_LOG_ANALYTICS_SHARED_KEY")
    print("\nüîí SECURITY:")
    print("  ‚úÖ Logs shipped to immutable cloud storage")
    print("  ‚úÖ Cannot be deleted by compromised server")
    print("  ‚úÖ HMAC signatures prevent tampering")
    print("  ‚úÖ Automatic retry with exponential backoff")
    print("  ‚úÖ Falls back to local if cloud unavailable")
    print("="*80)
